{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "0d2e6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importamos la libreria \"pandas\" para hacer uso de df's\n",
    "import nltk #importamos la libreria para el procesamiento\n",
    "from nltk.corpus import stopwords #usamos la libreria stopwords para eliminarlas en el proceso\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "a49d810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clase que usaremos para las funciones (normalizacion)\n",
    "\n",
    "class tukey_tools: #usamos una clase para tener todas las funciones almacenadas aqui\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def to_lower(self, texto: str)->str: #convierte el texto a puras minusculas\n",
    "        return texto.lower()\n",
    "    \n",
    "    def remove_special(self, text:str)->str: #quita los caracteres especiales, solo guarda letras y espacios\n",
    "        result = \"\"\n",
    "        for letter in text:\n",
    "            if letter.isalpha() or letter.isspace():\n",
    "                result += letter\n",
    "            else:\n",
    "                continue\n",
    "        return result\n",
    "    \n",
    "    def tokenize(self, text: str)->list: #tokenizamos cada palabra del texto (guardamos una por una en una lista)\n",
    "        tokens = list()\n",
    "        j = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] == ' ':\n",
    "                tokens.append(text[j:i])\n",
    "                j = i+1\n",
    "            else:\n",
    "                continue\n",
    "        tokens.append(text[j:])\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stop_words(self, tokens: list)->list: #eliminamos las palabras que no aportan ningun significado\n",
    "        no_stopwords = list()\n",
    "        for i in tokens:\n",
    "            if i not in stop_words and i != 'im':\n",
    "                no_stopwords.append(i)\n",
    "            else:\n",
    "                continue\n",
    "        return no_stopwords\n",
    "    \n",
    "    def words(self, frase: list, emocion: str)->dict: #crea un diccionario con palabras (tambien repetidas) de cada emocion \n",
    "        words = {} #diccionario vacio\n",
    "        for i in range(len(emocion)): #usamos la longitud de la columna \"emocion\" para poder usar indices\n",
    "            if emocion[i] not in words: #si la emocion ya se encuentra en las claves del diccionario entonces no se agrega\n",
    "                words[emocion[i]] = [] \n",
    "            words[emocion[i]] += [frase[i]] #agrega las listas de la columna \"frase\" a la emocion que corresponda\n",
    "        return words\n",
    "    \n",
    "    def count(self, palabras: dict)->dict: #cuenta cuantas veces se repite cada palabra\n",
    "        for emotion in palabras.keys(): #recorremos cada emocion\n",
    "            reps = {} #creamos un diccionario que se actualiza a vacio cada que cambia de emocion\n",
    "            for word in palabras[emotion]: #recorremos las palabras de cada clave\n",
    "                if word in reps: #si ya existe en el diccionario aumentamos su valor en uno\n",
    "                    reps[word] = reps[word]+1\n",
    "                else:\n",
    "                    reps[word] = 1 #si no existe lo dejamos como =1 ya que solo esta una vez\n",
    "            palabras[emotion] = reps #agregamos el diccionario a cada clave del diccionario principal\n",
    "        return palabras\n",
    "    \n",
    "    def maxxx(self, palabras: dict)->dict: #funcion que extrae las 3 palabras mas comunes de cada clave\n",
    "        maxx = [] #lista vacia\n",
    "        for emotion in palabras.keys(): #se recorren las emociones\n",
    "            maxx = [] #se actualiza la lista a vacia\n",
    "            for i in range(3): #se recorre 3 veces para obtener 3 palabras distintas\n",
    "                max_count = max(palabras[emotion], key = palabras[emotion].get) #se determina la palabra con mayor numero de repeticiones\n",
    "                maxx.append(max_count) #agregamos la palabra a la lista\n",
    "                del palabras[emotion][max_count] #borramos el valor del diccionario principal para obtener la segunda con mayor cantidad\n",
    "            palabras[emotion] = maxx #asignamos las 3 palabras de la emocion al diccionario\n",
    "        return palabras\n",
    "    \n",
    "    def emotions(self, emociones: str)->list: #devuelve las emociones sin repetir\n",
    "        ems = []\n",
    "        for emotion in emociones:\n",
    "            if emotion not in ems:\n",
    "                ems.append(emotion)\n",
    "            else:\n",
    "                continue\n",
    "        return ems\n",
    "    \n",
    "    def inter_words(self, interseccion: dict, ems: list)->dict: #determina si hay palabras contenidas en dos o mas emociones y las almacena en su respectiva emocion\n",
    "        inter = {}\n",
    "        for emotion in interseccion.keys():\n",
    "            reps = []\n",
    "            ems.remove(emotion)\n",
    "            for word in interseccion[emotion]:\n",
    "                for em in ems:\n",
    "                    if word in interseccion[em] and word not in reps:\n",
    "                        reps.append(word)\n",
    "            ems.append(emotion)\n",
    "            inter[emotion] = reps\n",
    "        return inter\n",
    "        \n",
    "    def max_corpus(self, palabras: list)->str: #determina la palabra mas mencionada en el corpus \n",
    "        words = {}\n",
    "        for word in palabras:\n",
    "            if word in words:\n",
    "                words[word] = words[word]+1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "        most_rep = max(words, key = words.get)\n",
    "        return most_rep\n",
    "            \n",
    "    def long(self, palabras: dict)->dict: #calcula la media del total de oraciones de cada emocion \n",
    "        suma = 0\n",
    "        aux = 0\n",
    "        promedio = 0\n",
    "        long = {}\n",
    "        for emotion in palabras.keys():\n",
    "            aux = 0\n",
    "            suma = 0\n",
    "            for line in palabras[emotion]:\n",
    "                aux += 1\n",
    "                suma += len(line)\n",
    "            promedio = suma/aux\n",
    "            long[emotion] = promedio\n",
    "        return long\n",
    "    \n",
    "    def exclusivas(self, ex: dict, ems: list)->dict:\n",
    "        excl = {}\n",
    "        w = []\n",
    "        elims = []\n",
    "        for emotion in ex.keys():\n",
    "            w = []\n",
    "            ems.remove(emotion)\n",
    "            for word in ex[emotion]:\n",
    "                for em in ems:\n",
    "                    if word not in ex[em] and word not in elims:\n",
    "                        if word not in w:\n",
    "                            w.append(word)\n",
    "                    elif word in ex[em] and word in w:\n",
    "                        w.remove(word)\n",
    "                        elims.append(word)\n",
    "            excl[emotion] = w\n",
    "            ems.append(emotion)\n",
    "        return excl  \n",
    "    \n",
    "    def significado(self, interseccion: dict): #busca las palabras que se repiten en las emociones, ya que se usan en diferentes contextos\n",
    "        reps = {}\n",
    "        for emotion in interseccion.keys():\n",
    "            for word in interseccion[emotion]:\n",
    "                if word not in reps:\n",
    "                    reps[word] = [emotion]\n",
    "                elif word in reps and emotion not in reps[word]:\n",
    "                    reps[word].append(emotion)\n",
    "        return reps\n",
    "\n",
    "    \n",
    "    def tf_idf(self, tokens, vocab): #representacion numerica de los datos\n",
    "        no_docs = len(tokens)\n",
    "        # IDF\n",
    "        idf = dict()\n",
    "        for word in vocab:\n",
    "            n_t = sum(1 for document in tokens if word in document)\n",
    "            idf[word] = math.log(no_docs/(1+n_t))+1\n",
    "        # TF-IDF\n",
    "        tf_idf_matrix = []\n",
    "        for doc in tokens:\n",
    "            doc_len = len(doc)\n",
    "            tf_idf_vector = []\n",
    "            for word in vocab:\n",
    "                tf = doc.count(word) / doc_len\n",
    "                tf_idf_val = tf * idf[word]\n",
    "                tf_idf_vector.append(tf_idf_val) \n",
    "            tf_idf_matrix.append(tf_idf_vector)\n",
    "        return np.array(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "7f37d38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Frase  Emocion\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness\n",
      "1          im updating my blog because i feel shitty  sadness\n",
      "2  i never make her separate from me because i do...  sadness\n",
      "3  i left with my bouquet of red and yellow tulip...      joy\n",
      "4    i was feeling a little vain when i did this one  sadness\n"
     ]
    }
   ],
   "source": [
    "#transformacion del archivo en formato txt a csv\n",
    "import pandas as pd\n",
    "txt = open(\"test.txt\", \"r\", encoding=\"utf-8\") #abrimos el archivo en forma de lectura\n",
    "csv = open(\"examen.csv\", \"w\", encoding=\"utf-8\") #abrimos en archivo en modo escritura\n",
    "csv.write('Frase;Emocion\\n') #se imrpime esto (se busca que funcione como encabezado para el df)\n",
    "for line in txt:\n",
    "    csv.write(line) #copiamos el contenido de txt a csv \n",
    "txt.close() #cerramos ambos archivos\n",
    "csv.close()\n",
    "\n",
    "df = pd.read_csv('examen.csv', sep=';')#representamos el archivo en df, separando las columnas por ';'\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "afc41999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Frase  Emocion\n",
      "0        [feeling, rather, rotten, ambitious, right]  sadness\n",
      "1                     [updating, blog, feel, shitty]  sadness\n",
      "2  [never, make, separate, ever, want, feel, like...  sadness\n",
      "3  [left, bouquet, red, yellow, tulips, arm, feel...      joy\n",
      "4                       [feeling, little, vain, one]  sadness\n"
     ]
    }
   ],
   "source": [
    "#normalizacion de texto\n",
    "herramientas = tukey_tools() #creamos una instancia para la clase \n",
    "\n",
    "#convertir todo a minusculas\n",
    "df['Frase'] = df['Frase'].apply(herramientas.to_lower) \n",
    "df['Emocion'] = df['Emocion'].apply(herramientas.to_lower) \n",
    "\n",
    "#eliminamos caracteres especiales\n",
    "df['Frase'] = df['Frase'].apply(herramientas.remove_special) \n",
    "\n",
    "#tokenizamos palabra por palabra\n",
    "df['Frase'] = df['Frase'].apply(herramientas.tokenize) \n",
    "\n",
    "#eliminamos las stop words (no aportan ningun significado)\n",
    "df['Frase'] = df['Frase'].apply(herramientas.remove_stop_words) \n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "2c83437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representacion en tf-idf\n",
    "tokens = df['Frase'].tolist()\n",
    "vocab = list(set(word for line in tokens for word in line))\n",
    "tf = herramientas.tf_idf(tokens, vocab)\n",
    "emsss = [] \n",
    "emsss = df['Emocion'].tolist()\n",
    "tfidf = pd.DataFrame(tf, columns = vocab)\n",
    "tfidf.insert(0, 'Emocion', emsss)\n",
    "\n",
    "# Guardar DataFrame a archivo CSV\n",
    "tfidf.to_csv('tfidf_output.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "49dfd6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Palabras mas comunes de sadness: ['feel', 'feeling', 'like']\n",
      "Palabras mas comunes de joy: ['feel', 'feeling', 'like']\n",
      "Palabras mas comunes de fear: ['feel', 'feeling', 'like']\n",
      "Palabras mas comunes de anger: ['feel', 'feeling', 'like']\n",
      "Palabras mas comunes de love: ['feel', 'feeling', 'like']\n",
      "Palabras mas comunes de surprise: ['feel', 'feeling', 'like']\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#1- Palabras mas comunes de cada emocion\n",
    "conteo = {}\n",
    "conteo =  herramientas.words(df['Frase'], df['Emocion']) #guardamos cada lista de la columna frase como valor de cada clave a la que pertenece (emocion)\n",
    "for emocion in conteo.keys(): #recorremos las claves del diccionario (emociones)\n",
    "    words = [] #creamos una lista donde se van  almacenar los elementos de las listas segun su clave\n",
    "    for palabras in conteo[emocion]: #recorremos los valores de la emocion\n",
    "        words.extend(palabras) #extendemos la lista, con el fin de tener una sola y no varias \n",
    "    conteo[emocion] = words #asignamos la anterior lista como valor de nuestra clave actual \n",
    "#print(conteo)comprobamos que de la salida esperada\n",
    "\n",
    "conteo = herramientas.count(conteo) #llamamos a la funcion que se encarga del conteo para saber la cantidad de veces que se repite cada palabra\n",
    "#print(conteo) comprobamos la salida\n",
    "\n",
    "conteo = herramientas.maxxx(conteo) #llamamos a la funcion donde se obtienen las 3 palabras mas comunes de cada emocion\n",
    "print(\"\\n\")\n",
    "\n",
    "for emotion in conteo.keys():\n",
    "    print(f\"Palabras mas comunes de {emotion}: {conteo[emotion][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deacd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness: ['feeling', 'rather', 'rotten', 'ambitious', 'right']\n",
      "joy: ['left', 'bouquet', 'red', 'yellow', 'tulips']\n",
      "fear: ['cant', 'walk', 'shop', 'anywhere', 'feel']\n",
      "anger: ['felt', 'anger', 'end', 'telephone', 'call']\n",
      "love: ['find', 'odd', 'position', 'feeling', 'supportive']\n",
      "surprise: ['feel', 'little', 'stunned', 'imagine', 'folks']\n",
      "\n",
      "\n",
      "Las palabras de sadness que se intersectan con otras emociones, son: ['feeling', 'rather', 'right', 'updating', 'blog']\n",
      "Las palabras de joy que se intersectan con otras emociones, son: ['left', 'red', 'yellow', 'feeling', 'slightly']\n",
      "Las palabras de fear que se intersectan con otras emociones, son: ['cant', 'walk', 'feel', 'uncomfortable', 'particularly']\n",
      "Las palabras de anger que se intersectan con otras emociones, son: ['felt', 'anger', 'end', 'call', 'feel']\n",
      "Las palabras de love que se intersectan con otras emociones, son: ['find', 'odd', 'feeling', 'feel', 'like']\n",
      "Las palabras de surprise que se intersectan con otras emociones, son: ['feel', 'little', 'imagine', 'folks', 'working']\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#2- Interseccion de palabras de todas las emociones\n",
    "interseccion = {} #creamos un diccionario vacio\n",
    "interseccion = herramientas.words(df['Frase'], df['Emocion']) \n",
    "#convertimos todas las sublistas de cada emocion en una sola lista usando extend\n",
    "for emocion in interseccion.keys(): \n",
    "    words = [] \n",
    "    for palabras in interseccion[emocion]: \n",
    "        words.extend(palabras) \n",
    "    interseccion[emocion] = words \n",
    "for emotion in interseccion.keys():\n",
    "    print(f\"{emotion}: {interseccion[emotion][:5]}\") #solo se imprimen algunos para representar\n",
    "print(\"\\n\")\n",
    "\n",
    "ems = [] #creamos una lista vacia para almacenar las emociones\n",
    "ems = herramientas.emotions(df['Emocion']) #llamamos a la funcion\n",
    "#print(f\"{ems}\\n\")#comprobamos la salida\n",
    "\n",
    "inter = {}\n",
    "inter = herramientas.inter_words(interseccion, ems) #llama a la funcion donde se determinan las intersecciones\n",
    "for emotion in inter.keys():\n",
    "    print(f\"Las palabras de {emotion} que se intersectan con otras emociones, son: {inter[emotion][:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "6a7f7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra: feeling: ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
      "Palabra: rather: ['sadness', 'joy', 'fear', 'anger']\n",
      "Palabra: right: ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
      "Palabra: updating: ['sadness', 'anger']\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#3- Palabras con signficado diferente \n",
    "significa = {}\n",
    "intersect = {}\n",
    "ems_2 = []\n",
    "ems_2 = herramientas.emotions(df['Emocion'])\n",
    "intersect = herramientas.inter_words(interseccion, ems_2)\n",
    "significa = herramientas.significado(intersect)\n",
    "\n",
    "for word in significa.keys():\n",
    "    i += 1\n",
    "    if i < 5:\n",
    "        print(f\"Palabra: {word}: {significa[word]}\")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "e4d1889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra mas usada en el corpus es: feel\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#4- Palabras mas frecuentes de todo el corpus\n",
    "mayor_corpus = \"\"\n",
    "lista = []\n",
    "for line in df['Frase']:\n",
    "    lista.extend(line) #guardamos todas las listas (de las oraciones) generadas en una sola para recorrerla\n",
    "mayor_corpus = herramientas.max_corpus(lista) #llamamos a la funcion\n",
    "print(f\"La palabra mas usada en el corpus es: {mayor_corpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "09724cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud promedio de cada oracion correspondiente a sadness es: 9.08605851979346\n",
      "La longitud promedio de cada oracion correspondiente a joy es: 9.130935251798562\n",
      "La longitud promedio de cada oracion correspondiente a fear es: 8.700892857142858\n",
      "La longitud promedio de cada oracion correspondiente a anger es: 9.374545454545455\n",
      "La longitud promedio de cada oracion correspondiente a love es: 9.59748427672956\n",
      "La longitud promedio de cada oracion correspondiente a surprise es: 9.318181818181818\n",
      "\n",
      "\n",
      "La emocion cuyas oraciones tienen el mayor promedio es love, mientras que la de menor promedio es fear\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#5- Comparar la longitud promedio de las oraciones por clase\n",
    "palabras = {}\n",
    "longitud = {}\n",
    "palabras = herramientas.words(df['Frase'], df['Emocion']) #volvemos a usar el diccionario que nos devuelve cada oracion (ya procesada) para calcular el promedio\n",
    "longitud = herramientas.long(palabras) #llamamos a la funcion para calcular la media\n",
    "for emotion in longitud.keys():\n",
    "    print(f\"La longitud promedio de cada oracion correspondiente a {emotion} es: {longitud[emotion]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"La emocion cuyas oraciones tienen el mayor promedio es {max(longitud, key = longitud.get)}, mientras que la de menor promedio es {min(longitud, key = longitud.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "acbb0e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las palabras exclusivas de sadness son: ['rotten', 'ambitious', 'shitty', 'separate', 'ashamed']\n",
      "Las palabras exclusivas de joy son: ['bouquet', 'yellow', 'tulips', 'arm', 'clung']\n",
      "Las palabras exclusivas de fear son: ['shop', 'anywhere', 'pay', 'deepens', 'invaded']\n",
      "Las palabras exclusivas de anger son: ['telephone', 'jest', 'pre', 'menstrual', 'walrus']\n",
      "Las palabras exclusivas de love son: ['position', 'supportive', 'naughty', 'border', 'foreigner']\n",
      "Las palabras exclusivas de surprise son: ['stunned', 'shocked', 'handed', 'billiards', 'contractions']\n"
     ]
    }
   ],
   "source": [
    "#Analisis\n",
    "#6- Palabras exclusivas de cada emocion\n",
    "ems_dos = []\n",
    "ems_dos = herramientas.emotions(df['Emocion'])\n",
    "#print(ems_dos)\n",
    "\n",
    "excl = {}\n",
    "excl = herramientas.words(df['Frase'], df['Emocion']) \n",
    "for emocion in excl.keys(): \n",
    "    words = [] \n",
    "    for palabras in excl[emocion]: \n",
    "        words.extend(palabras) \n",
    "    excl[emocion] = words \n",
    "#print(excl) \n",
    "\n",
    "exclusiv = herramientas.exclusivas(excl, ems_dos)\n",
    "for emotion in exclusiv.keys():\n",
    "    print(f\"Las palabras exclusivas de {emotion} son: {exclusiv[emotion][:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "a5b0b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusiones\n",
    "#sadness: Las palabras relacionadas a esta emocion reflejan frustracion y desagrado, interpreto los sentimientos de alguien por haber perdido algo valioso o no cumplir un objetivo\n",
    "#joy: Al menos en las palabras que describen esta emocion destacan las que hacen alucion a flores y a cosas sin un compromiso, puede ser una relacion joven que aun no es seria\n",
    "#Fear: Las palabras correspondientes al miedo se caracterizan por la inseguridad o temerosidad, tambien algunas palabras mencionan acciones como pagar, se puede relacionar a no querer gastar dinero\n",
    "#Anger: Representan frustracion acerca sobre vivencias personales o con 3ras personas\n",
    "#Love: Son palabras que se refieren de muy buena forma a una persona, tal como ser solidario, se pueden relacionar mas que nada a una relacion y al romance\n",
    "#Surprise: Algunas palabras resaltan el impacto al recirbir una gran sorpresa, un ejemplo es \"shocked\" o a entregar o recibir cosas\n",
    "\n",
    "#insights generales: desde mi perspectiva, fue un ejercicio muy interesante y divertido, realmente se nota la forma en la que ayuda la normalizacion de los datos,\n",
    "        #se puede ver como al inicio hay muchas palabras que no aportan ninguna importancia o significado pero despues de este proceso, el numero\n",
    "        #decrementa considerablemente, permitiendo una mayor facilidad en la manipulacion de estos datos, ya sea para su analisis como en este caso,\n",
    "        #tambien nos permitio conocer las palabras que toman un diferente significado dependiendo su contexto, como hay palabras caracteristicas y exclusivas\n",
    "        #para cada emocion, realmente fue muy agradable realizar esta prueba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
